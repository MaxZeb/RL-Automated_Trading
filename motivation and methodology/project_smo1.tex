\documentclass[12pt]{article}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{amsmath}
\setstretch{1.5}

%\DeclareMathSizes{12}{14}{10}{10}
\usepackage[margin=2.5cm]{geometry}    % How to set margins - optimized for 2.5cm      



%opening
\title{}
\author{}

\begin{document}

\maketitle


\section{Motivation}
Reinforcement Learning(RL) is considered as a third paradigm alongside unsupervised and supervised learning. But nevertheless, unlike the other approaches RL considers the whole problem of goal-directed agent that interacts in an uncertain environment. This involves learning what is necessary to do in order to maximize a numerical reward signal. 

The most important distinguishing features of a RL problem are: 1. they behave as closed-loop problems given that its learned actions influence later inputs, 2. learners must try different operations to discover which strategy yields the most reward, and 3. actions may affect next situations and all subsequent rewards.\footnote{Sutton and Barto, 2014}

Among its main applications are: resources management in computer clusters, games, traffic light control, robotics, etc. The aim of this project is to apply the RL techniques to make decisions in the stock market given that it involves the interaction of an active agent that has to make decisions based in imperfect information environment while is interacting with other market participants. Some previous findings indicate that RL can be successfully applied to the portfolio problem and its performance exceeds the supervised learning approach\footnote{Neuneier, 1996} and Q-learning algorithm operates better than kernel-based methods\footnote{Bertolutzzo and Corazza, 2012}.

The approach in this paper to the stock market prediction problem is to apply the Q-learning algorithm that is a RL method that uses the principles of Dynamic Programming. The paper is organized with the following sections: first Data, the explanation of stock market data utilized for the purpose of analysis; follow by Methodology, an overview of what is Reinforcement Learning, Q-learning and formulation of the problem; next section Results, talks about the implementation of RL to the stock market problem and obtained results; and the last section Conclusion, explains findings and conclusions of the project.             


\section{Methodology}
\subsection{Overview}
Before delving into the specifics of employing reinforcement learning to the problem of automated trading, it will be informative to discuss the general theory and its underpinning principles. 

Reinforcement learning aims to maximise a given reward signal by undertaking certain actions (a restricted space). In this framework, an agent must take the environment state as input and take actions that alter the future state. A measurable goal related to the environment is also necessary in the problem formulation. Beyond this, each reinforcement learning problem contains four subelements: a \textit{policy}, a \textit{reward signal} and a \textit{value function}.\footnote{sutton book }

The policy defines the agent's actions in different environment states. The reward signal defines the goal and should be maximised over the course of the learning process.The value function maps the current state to a value so the agent can make optimal longer run decisions. It can be seen as the expected total future reward that can be obtained beginning from that state. Most of the challenges associated with the implementation of reinforcement learning derive from the estimation of the value function.

Formally, we construct a Markov Decision Process (MDP)
In an ideal situation, we would have access to the value function directly in tabular form when we have a tractable action and state space.

At a sequence of discrete time steps $t = 0,1,2,3...$, the agent interacts with the environment. At each step $t$, the agent receives state information $S _ { t } \in \mathcal{S}$ and performs an action $A _ { t } \in \mathcal { A } ( s )$. As a consequence of the action, the agent receives a reward $R _ { t + 1 } \in \mathcal { R } \subset \mathbb { R }$ and transitions to a new state $S _ { t + 1 }$.

In the context of a \textit{Markov} decision process, the future rewards ($R_{t}$) and states ($S_{t}$) only depend on the previous state and action. 

The general reinforcement learning paradigm involves find an optimal policy $\pi$ to maximise the expected discounted return. The discount factor is required to ensure that rewards in the distant future are less valuable than current rewards.

$$
G _ { t } \doteq R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \cdots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }
$$

Value and action-value functions allow the actions of the agent to be assessed under the implementation of a certain policy. The value function and action-value functions respectively are defined below:

$$
\begin{aligned}
v _ { \pi } ( s ) \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right] = \mathbb { E } _ { \pi } \left[ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 } | S _ { t } = s \right] , \text { for all } s \in \mathcal{S} \\
q _ { \pi } ( s , a ) \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right] = \mathbb { E } _ { \pi } \left[ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 } | S _ { t } = s , A _ { t } = a \right]
\end{aligned}
$$ 

Ideally, the value function is decomposed into the following (known as the \textit{Bellman's Equation}):

$$
\begin{aligned} v _ { \pi } ( s ) & \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right] \\ & = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ & = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } } \sum _ { r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma \mathbb { E } _ { \pi } \left[ G _ { t + 1 } | S _ { t + 1 } = s ^ { \prime } \right] \right] \\ & = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } , r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma v _ { \pi } \left( s ^ { \prime } \right) \right] , \quad \text { for all } s \in S \end{aligned}
$$

Both expressions relate to a specific state and action taken at any time $t$.

A reinforcement learning problem principally involves pursuing the optimal policy $\pi$ which is said to maximise the value and action-value functions:

$$
\begin{aligned}
v _ { * } ( s ) &\doteq \max _ { \pi } v _ { \pi } ( s )\\
q _ { * } ( s , a ) &\doteq \max _ { \pi } q _ { \pi } ( s , a )
\end{aligned}
$$

In the most simple cases where the value and action-value functions are specified, dynamic programming can be used to derive the optimal policy $\pi$.







In the algorithmic trading problem, the value function cannot be ascertained easily in this way. To deal with these situations and arbitrarily large state space, approximate solution methods must be used. This is known as a partially observable Markov decision process as the state is only observed indirectly and cannot be fully known (we cannot know the trading behavior of other agents for example) and we do not have access to the transition probabilities between states.

Q-Learning is a technique whereby the value functions are repeatedly estimated based on the rewards of our actions and assumes no prior model specification.


\subsection{Q-Learning} 

Q-learning attempts to estimate $q_{*}$ (optimal action-value function) without any regard for the policy followed.From a high level perspective, The Q-Learning algorithm proceeds by randomly initialising $q$, perform actions, measure reward and update $q$ accordingly. The final output after a training period should be a stable approximation of the $q_{*}$ table.

$$
\begin{array} { l } { \text { Algorithm parameters: step size } \alpha \in ( 0,1 ] , \text { small } \varepsilon > 0 } \\ { \text { Initialize } Q ( s , a ) , \text { for all } s \in \delta ^ { + } , a \in \mathcal { A } ( s ) , \text { arbitrarily except that } Q ( \text {terminal} , \cdot ) = 0 }  \\
\text{ Loop for each episode:} \\
\quad\text{ Initialize } S \\
\indent \text{ Loop for each step of episode:} \\
 \indent{ \text { Choose } A \text { from } S \text { using policy derived from } Q ( \text { e.g. } , \varepsilon \text { -greedy } ) } \\ \indent{ \text { Take action } A , \text { observe } R , S ^ { \prime } } \\ \indent { \hphantom{1}Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left[ R + \gamma \max _ { a } Q \left( S ^ { \prime } , a \right) - Q ( S , A ) \right] } \\ \indent{ \hphantom{1} S \leftarrow S ^ { \prime } }\\
\end{array}
$$ 


Notice the Bellman equation appearing in the update phase of the algorithm.

\subsubsection{Deep Q Learning}
An extension of this idea (which we aim to employ) is to use Neural Networks to approximate the Q-function.

A Neural Network is an appropriate tool for our use case due to the infinite nature of the state space. Estimating a table for each possible state would be excessive in regards to memory requirements.

To train the Neural Network on the state space, we must define a loss function. As The Bellman Equation defines the optimal result, we can use this to calculate our loss as follows:

$$
\begin{aligned}
\hat { Q } ( s , a ) &= R ( s , a ) + \gamma \max _ { a ^ { \prime } \in A } Q ( s , a ) \\
\text {Loss} &= \| Q - \hat { Q } \| _ { 2 }
\end{aligned}
$$







\subsection{Problem formulation - Algorithmic Trading}
Now we must formulate the trading problem as a Markov Decision Process and define the states, actions and rewards.

\begin{itemize}
	\item State: $\mathcal{S} = \{prices,holdings,balance\} $
	\item Actions: $\mathcal{A} = \{buy,sell,hold\}$
	\item Rewards: $R_{t} \in \mathcal{R}$ can be defined as the change in the portfolio value due to an action $A_{t}$
	\item Policy: $\pi$ which is governs the trading strategy at state $S$
	\item Action-value function: $q_{\pi}(s,a)$ as defined above. The expected reward we obtain by following policy $\pi$, choosing action $A$ while in state $S$.
\end{itemize}










\end{document}