\documentclass[12pt, authoryear]{elsarticle}
\makeatletter
\def\ps@pprintTitle{%
	\let\@oddhead\@empty
	\let\@evenhead\@empty
	\def\@oddfoot{}%
	\let\@evenfoot\@oddfoot}
\makeatother
%\usepackage{lmodern}
% My spacing
\usepackage{setspace}
\setstretch{1.5}
\usepackage{multirow}
%\DeclareMathSizes{12}{14}{10}{10}
\usepackage[margin=2.5cm]{geometry}    % How to set margins - optimized for 2.5cm      
\usepackage[capposition=top]{floatrow}

% See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   			% ... or a4paper or a5paper or ... 
\usepackage{enumitem}
\usepackage{mathtools}
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    			% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}						% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{flafter}			
\usepackage{setspace}
%\linespread{1.5}
\usepackage[font={}]{caption}
\usepackage[bottom]{footmisc}
\usepackage[capposition=top]{floatrow}   %figure notes
\usepackage{lscape}
%math packages 
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx,epsf,subfigure}
\usepackage{pstricks,pst-node,psfrag}
\usepackage{amsthm,amssymb,amsmath}
\usepackage{amsmath,bm}

%mathnotes
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bX}{\mbox{\boldmath $X$}}
\newcommand{\bY}{\mbox{\boldmath $Y$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\x}{\textsc{\textbf{x}}}
\newcommand{\xx}{\textsc{x}}
\definecolor{aurometalsaurus}{rgb}{0.43, 0.5, 0.5}
%add figure 
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{hyperref}
\usepackage[round]{natbib}
\definecolor{ashgrey}{rgb}{0.7, 0.75, 0.71}
\usepackage{soul}
\usepackage{animate}

\def\bibsection{\section{References}} %%% Make "References" appear before bibliography
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{tablefootnote}
\usepackage{lscape} 
\usepackage{animate}

\renewcommand{\contentsname}{Table of Contents} % change name from Contents to Table of Contents

\usepackage{titlesec}

\setcounter{secnumdepth}{4}

%_______________________________________________________________________________________________________%
%_______________________________________________________________________________________________________%
%\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
%\usepackage{graphicx,multirow}
\usepackage{xcolor,colortbl}
\usepackage{xcolor}
%\usepackage{graphicx,multirow}
\usepackage[capposition=top]{floatrow}
\setcounter{secnumdepth}{4}
\usepackage{tikz}
\begin{document}
	
	\begin{frontmatter}  %
		
		\title{THE TRADING BOT\ \vspace{0.5cm} \large \\
			REINFORCEMENT LEARNING FOR AUTOMATED TRADING
		}
		
		\author[Add1]{Reid Falconer}
		\ead{reid.falconer@bracelonagse.eu}
		
		\author[Add1]{Sam MacIntyr}
		\ead{sam.macintyre@barcelonagse.eu}
		
		\author[Add1]{Hector Cano}
		\ead{hector.cano@barcelonagse.eu}

		\author[Add1]{Maximilian Zebhauser}
		\ead{maximilian.zebhauser@barcelonagse.eu}

		
		\address[Add1]{Barcelona Graduate School of Economics, Barcelona, Spain}
		%\address[Add2]{Some other Institution, Cape Town, South Africa}
		
		%\cortext[cor]{Corresponding author: Nico Katzke}
		
		%\begin{abstract}
		%\small{
		%Abstract to be written here. 
		%}
		%\end{abstract}
		
		%\vspace{1cm}
		
		\begin{keyword}
			\footnotesize{
				Line Search \sep  Gradient Descent \sep Newton's Method \\ \vspace{0.3cm}
				%\textit{JEL classification} L250 \sep L100
			}
		\end{keyword}
		\vspace{0.5cm}
	\end{frontmatter}
	
	\headsep 25pt % So that header does not go over title

\section{Motivation}
Reinforcement Learning (RL) is considered as a third paradigm alongside unsupervised and supervised learning. Nevertheless, unlike the other approaches, RL considers the whole problem of a goal-directed agent that interacts in an uncertain environment. This involves learning what actions are necessary to take in order to maximize a numerical reward signal. 

The most important distinguishing features of a RL problem are:
\begin{enumerate}
	\item They behave as closed-loop problems given that its learned actions influence later inputs
	\item Learners must try different operations to discover which strategy yields the most reward
	\item Actions may affect next situations and all subsequent rewards\footnote{Sutton and Barto, 2014}
\end{enumerate}

Among its main applications are: resources management in computer clusters, games, traffic light control, robotics, etc. The aim of this project is to apply the RL techniques to make decisions in the stock market given that it involves the interaction of an active agent that has to make decisions based in imperfect information environment while also interacting with other market participants. Some previous findings indicate that RL can be successfully applied to the portfolio problem and its performance exceeds the supervised learning approach\footnote{Neuneier, 1996} and Q-learning algorithm operates better than kernel-based methods\footnote{Bertolutzzo and Corazza, 2012}.

The approach in this paper to the stock market prediction problem is to apply the Q-learning algorithm that is a RL method that uses the principles of Dynamic Programming. The paper is organized with the following sections: Motivation followed by Methodology, an overview of what is Reinforcement Learning, Q-learning and formulation of the problem; next section Results, talks about the implementation of RL to the stock market problem and obtained results; and the last section Conclusion, explains findings and conclusions of the project.  



\section{Methodology}
\subsection{Overview}
Before delving into the specifics of employing reinforcement learning to the problem of automated trading, it will be informative to discuss the general theory and its underlying principles. 

Reinforcement learning aims to maximise a given reward signal by undertaking certain actions (in a restricted space). In this framework, an agent must take the state of the environment as input and take actions to alter the future state. A measurable goal related to the environment is also necessary in the problem formulation. Beyond this, each reinforcement learning problem contains four sub-elements: a \textit{policy}, a \textit{reward signal} and a \textit{value function}.\footnote{sutton book }

The policy defines the agent's actions in different environment states. The reward signal defines the goal and should be maximised over the course of the learning process. The value function maps the current state to a value so the agent can make optimal longer run decisions. It can be seen as the expected total future reward that can be obtained beginning from that state. Most of the challenges associated with the implementation of reinforcement learning derive from the estimation of the value function.

Formally, we construct a Markov Decision Process (MDP).
In an ideal situation, we would have access to the value function directly in tabular form when we have a tractable action and state space.

\begin{figure}[h!]
\centering
\caption{Agent and Environment Interaction}
\includegraphics[clip, angle=0, width=12cm]{figures/Loop.png}
\label{fig:1}
\end{figure}

At a sequence of discrete time steps $t = 0,1,2,3...$, the agent interacts with the environment. At each step $t$, the agent receives state information $S _ { t } \in \mathcal{S}$ and performs an action $A _ { t } \in \mathcal { A } ( s )$. As a consequence of the action, the agent receives a reward $R _ { t + 1 } \in \mathcal { R } \subset \mathbb { R }$ and transitions to a new state $S _ { t + 1 }$.

In the context of a \textit{Markov} decision process, the future rewards ($R_{t}$) and states ($S_{t}$) only depend on the previous state and action. 

The general reinforcement learning paradigm involves find an optimal policy $\pi$ to maximise the expected discounted return. The discount factor is required to ensure that rewards in the distant future are less valuable than current rewards.

$$
G _ { t } \doteq R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \cdots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }
$$

Value and action-value functions allow the actions of the agent to be assessed under the implementation of a certain policy. The value function and action-value functions respectively are defined below:

$$
\begin{aligned}
v _ { \pi } ( s ) \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right] = \mathbb { E } _ { \pi } \left[ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 } | S _ { t } = s \right] , \text { for all } s \in \mathcal{S} \\
q _ { \pi } ( s , a ) \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right] = \mathbb { E } _ { \pi } \left[ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 } | S _ { t } = s , A _ { t } = a \right]
\end{aligned}
$$ 

Ideally, the value function is decomposed into the following (known as the \textit{Bellman's Equation}):

$$
\begin{aligned} v _ { \pi } ( s ) & \doteq \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right] \\ & = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ & = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } } \sum _ { r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma \mathbb { E } _ { \pi } \left[ G _ { t + 1 } | S _ { t + 1 } = s ^ { \prime } \right] \right] \\ & = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } , r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma v _ { \pi } \left( s ^ { \prime } \right) \right] , \quad \text { for all } s \in S \end{aligned}
$$

Both expressions relate to a specific state and action taken at any time $t$.

A reinforcement learning problem principally involves pursuing the optimal policy $\pi$ which is is said to maximise the value and action-value functions:

$$
\begin{aligned}
v _ { * } ( s ) &\doteq \max _ { \pi } v _ { \pi } ( s )\\
q _ { * } ( s , a ) &\doteq \max _ { \pi } q _ { \pi } ( s , a )
\end{aligned}
$$

In the most simple cases where the value and action-value functions are specified, dynamic programming can be used to derive the optimal policy $\pi$.


In the algorithmic trading problem, the value function cannot be ascertained easily in this way. To deal with these situations and arbitrarily large state space, approximate solution methods must be used. This is known as a partially observable markov decision process as the state is only observed indirectly and cannot be fully known (we cannot know the trading behaviour of other agents for example) and we do not have access to the transition probabilities between states.

Q-Learning is a technique whereby the value functions are repeatedly estimated based on the rewards of our actions and assumes no prior model specification.

\subsection{Q-Learning} 

Q-learning attempts to estimate $q_{*}$ (optimal action-value function) without any regard for the policy followed.From a high level perspective, The Q-Learning algorithm proceeds by randomly initialising $q$, perform actions, measure reward and update $q$ accordingly. The final output after a training period should be a stable approximation of the $q_{*}$ table.

\begin{figure}[h!]
	\centering
	\caption{Agent and Environment Interaction}
	\includegraphics[clip, angle=0, width=14cm]{figures/psudo_code.png}
	\label{fig:1}
\end{figure}

Notice the Bellman equation appearing in the update phase of the algorithm.

\subsubsection{Deep Q Learning}
An extension of this idea (which we aim to employ) is to use neural networks to approximate the Q-function.

A neural network is an appropriate tool for our use case due to the infinite nature of the state space. Estimating a table for each possible state would be excessive in regards to memory requirements.

To train the neural network on the state space, we must define a loss function. As The Bellman Equation defines the optimal result, we can use this to calculate our loss as follows:

$$
\begin{aligned}
\hat { Q } ( s , a ) &= R ( s , a ) + \gamma \max _ { a ^ { \prime } \in A } Q ( s , a ) \\
\text {Loss} &= \| Q - \hat { Q } \| _ { 2 }
\end{aligned}
$$

In general, a partition of the data is used to train the neural network and approximate the $q_{*}$ function, then this is used as our action-value function for deciding the optimal policy.

A neural network with 3 hidden layers was implemented in Tensorflow in our case.


\subsection{Problem formulation - Algorithmic Trading}
Now we must formualate the trading problem as a Markov Decision Process and define the states, actions and rewards.

\begin{itemize}
	\item State: $\mathcal{S} = \{prices,holdings,balance\} $ where $prices$ refers to the current prices of all the stocks in our portfolio, $holdings$ the quantity of each stock held and $balance$ as the total portfolio value. In our simple variant, we are only trading one stock but include a history of prices also (default 200)
	\item Actions: $\mathcal{A} = \{buy,sell,hold\}$ For simplicity and computational tractability, we restrict our action space to buy 1 stock, sell 1 stock and hold.
	\item Rewards: $R_{t} \in \mathcal{R}$ can be defined as the change in the portfolio value due to an action $A_{t}$. Our Reward was defined as $R_{t} = balance_{t} - balance_{t-1}$
	\item Policy: $\pi$ which is governs the trading strategy at state $S$. We converge on the optimal policy by approximating the $q_{*}$ function.
	\item Action-value function: $q_{\pi}(s,a)$ as defined above. The expected reward we obtain by following policy $\pi$, choosing action $A$ while in state $S$. The action-value function is approximated by a neural network.
\end{itemize}

Initially we do not consider any transaction costs and only trade with a single stock (AAPL). Furthermore, a negative portfolio is not permissible ($balance_{t} \geq 0$ for all $t$).

In our primary model, the initial $balance_{0}$ is set to \$1000 and $price_{0}$ to the price of the stock at our chosen start time $t_{0}$ 

\section{Results}

Our neural network was set-up to learn from a 200 length rolling window (history) and as such, each state $S_{t}$ is a $200+2$ dimensional vector containing the history, the current budget ($balance_{t}$) and the current number of stocks held ($holdings_{t}$).

To ensure that the Q function was being learned in the correct way and trending towards an optimal policy, we train the neural network over multiple epochs. From the plot below, the final portfolio value achieved continues to increase, signalling convergence towards an optimal policy. The hyper-parameters were tweaked to derive maximum gain. 

\begin{figure}[h!]
	\centering
	\caption{Agent and Environment Interaction}
	\includegraphics[clip, angle=0, width=10cm]{figures/training.png}
	\label{fig:1}
\end{figure}

To evaluate the performance of the algorithm, we compared it to a standard \textit{Buy and Hold} strategy and to a randomised action strategy. The plots of the portfolio value over time below clearly demonstrate the superiority of Q-Learning, achieving a final portfolio profit of XXXX.

Despite the algorithm performing well, the \textit{Buy and Hold} strategy also performs similarly as AAPL is a strong stock with a stable upward trend.



To highlight the strength of Q-learning more effectively, we compared the algorithms behaviour on a more volatile stock, where \textit{Buy and Hold} should be less effective.

One of the noticeable strengths of the Q-Learning approach is its control of the volatility (captured by the Sharpe Ratio). The Sharpe ratio is defined as:
$$
\text {Sharpe Ratio} = \frac { R _ { p } - R _ { f } } { \sigma _ { p } }
$$
where $R_{f}$ is the risk free rate (assumed to be 8\%), $R_{p}$ the return of the portfolio and $\sigma_{p}$ is the standard deviation of the portfolio. A larger Sharpe Ratio indicates that the portfolio achieves a better return for its volatility.
In a practical setting, this would be an attractive feature for a potential investor.

\subsection{Non-trending stock comparison}

To illustrate how Q-Learning can out perform a \textit{Buy and Hold} strategy, we selected a stock with a less notable trend over the past 10 years (WWL - Wawel). Notably, the Q-learning algorithm clearly learns to buy low and sell high from its training, and out-performs \textit{Buy and Hold} dramatically.

\begin{table}[h!]
	\centering
	\caption{My caption}
	\label{my-label}
	\begin{tabular}{lcccccc} \\ \hline\hline
		& \multicolumn{3}{c}{AAPL} & \multicolumn{3}{c}{WLL} \\ \hline
		& DQL & BH & RAND & DQL & BH & RAND \\ \hline
		Accumulated Return(\%) & 1.67 & 1.33 & 0.59 & 0.64 & 0.41 & 0.18 \\
		Sharp Ratio & 0.29 & 0.22 & 0.26 & 0.25 & 0.20 & 0.09 \\\hline\hline
	\end{tabular}
\end{table}


\begin{figure}[h!]
	\centering
	\caption{Agent and Environment Interaction}
	\includegraphics[width=\linewidth]{figures/aapl_portfolio.png}
	\label{fig:1}
\end{figure}


\begin{figure}[h!]
	\centering
	\caption{Agent and Environment Interaction}
	\includegraphics[width=\linewidth]{figures/wwl_portfolio.png}
	\label{fig:1}
\end{figure}

\begin{figure}[h!]
  \begin{center} 
  	\caption{My caption}
  	\label{my-label}
	\includegraphics[scale=0.4]{figures/aapl_action_dql.png} \\
	\includegraphics[scale=0.4]{figures/aapl_action_rand.png} \\
	\includegraphics[scale=0.4]{figures/aapl_action_bh.png} \\
\end{center} 
\end{figure}

\begin{figure}[h!]
  \begin{center} 
  	\caption{My caption}
  	\label{my-label}
	\includegraphics[scale=0.4]{figures/wwl_action_dql.png} \\
	\includegraphics[scale=0.4]{figures/wwl_action_rand.png} \\
	\includegraphics[scale=0.4]{figures/wwl_action_bh.png} \\
\end{center} 
\end{figure}

\section{Conclusion}

The aim of the investigation was to implement reinforcement learning in the context of stock trading. A deep Q-Learning approach was used to approximate the Q-function (action-value function) and develop an trading policy. Through our experiments with AAPL and WLL stocks, we have managed to show that with minimal training and hyper-parameter tuning, our agent can outperform random actions and a \textit{Buy and Hold} strategy. However, the margin of improvement is variable depending on the presence of an upward trend in the stock price.  We have also demonstrated the agent's ability to manage volatility of the portfolio effectively. There are many possible future developments that could be explored such as using multiple stocks, modifying the reward function, performing online-learning and doing further hyper-parameter tuning. Furthermore, the initial budget appeared to be a strong determinant of the behaviour of our agent. A more thorough investigation into this and its implications for algorithmic trading would be an interesting extension.





\end{document}
